{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a brief run-through of the exercises, some cell might not execute without the Postgres database.\n",
    "\n",
    "All source code are in `.py` files, this notebook is for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model pipeline\n",
    "_1. Ingest this data into a local Postgres db_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `database` module contains functions to create a table and to connect to the DB. We have a configuration file `databse.ini` that stores the information (username, password, etc). Make a connection, then load the data file and use its columns to create a table named `car_loan_demo` with `customer_id` as primary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n"
     ]
    }
   ],
   "source": [
    "conn = connect()\n",
    "df = pd.read_csv('data/car_loan_trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, pd.read_csv('data/car_loan_trainset.csv'), 'car_loan_demo', 'customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then injest all the data into the DB then close the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.values.tolist():\n",
    "    keys = df.columns.tolist()\n",
    "    insert_row(conn, 'car_loan_demo', keys, row)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_2. Train a simple model (as a Data engineer, please do not spend much time on training - you can create a simple logistic regression model) and save it as `.pkl` locally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess module contains several functions to pre-process the data. We use a scaler to normaliza the data and save the fit scaler. This scaler can be loaded in the future. Alternatively, an encoder can be used for the categorical columns (id columns), but is skipped now. Instead, `id` columns are all dropped, but ideally we would do more feature engineering in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload 2\n",
    "df, x_columns = preprocess_pipeline(df, scale=True, scaler_save_path='saved/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move on to training the model, we use the preiously prcessed `df` object and split the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "\n",
    "label = 'loan_default'\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[x_columns], df[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a simple 3-layer NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.694692\n",
      " Val loss: 0.709099\n",
      " F1: 0.2750498622851173 | ACC: 0.44487272727272725 | Precision: 0.1777668651402615 | Recall:0.6075099643381582\n",
      "Epoch loss: 0.694700\n",
      " Val loss: 0.705741\n",
      " F1: 0.26920394540201253 | ACC: 0.46654545454545454 | Precision: 0.17652054615535376 | Recall:0.566813509544787\n",
      "Epoch loss: 0.694057\n",
      " Val loss: 0.703657\n",
      " F1: 0.2705573026756715 | ACC: 0.4854909090909091 | Precision: 0.17935748462064252 | Recall:0.5504510174113698\n",
      "Epoch loss: 0.692615\n",
      " Val loss: 0.701785\n",
      " F1: 0.26773988561745393 | ACC: 0.49716363636363636 | Precision: 0.1790748742650705 | Recall:0.5303125655548563\n",
      "Epoch loss: 0.692555\n",
      " Val loss: 0.700115\n",
      " F1: 0.2752273828103977 | ACC: 0.5102909090909091 | Precision: 0.18510207036339946 | Recall:0.5363960562198448\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "model_save_path = 'saved/model_demo'\n",
    "model, optimizer = train(x_train, y_train, x_test, y_test, lr=0.000005, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, optimizer, model_save_path)  # store the model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_3. Run predictions on the entire dataset and store the them into another Postgres table_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the following table on the DB:\n",
    "\n",
    "```CREATE TABLE predictions_demo (customer_id integer, prediction numeric, loan_default integer)```\n",
    "\n",
    "Then, make predictions with the model and join the predictions with the customer id and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import *\n",
    "all_predictions = pd.concat(\n",
    "        [\n",
    "            df['customer_id'],\n",
    "            pd.DataFrame([predict(model, list(row[1])) for row in df[x_columns].iterrows()]),\n",
    "            df[label],\n",
    "        ], 1,\n",
    "    )\n",
    "all_predictions.columns = ['customer_id', 'prediction', label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the DB and insert all predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n"
     ]
    }
   ],
   "source": [
    "connection = connect()\n",
    "for row in all_predictions.values.tolist():\n",
    "    keys = all_predictions.columns\n",
    "    insert_row(connection, 'predictions_demo', keys, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "### Expectation1:\n",
    "_1. Accept a HTTP POST request containing the features as input and return the prediction._\n",
    "\n",
    "\n",
    "The API design is as follows:\n",
    "```\n",
    "/predict: POST {'feature1': 'value1', 'feature2': 'value2', ...} (must be logged in to predict) Response prediction\n",
    "/register: POST {'username': ..., 'password': ***} Response status (success, already exists or error)\n",
    "/login: POST {'username': ..., 'password': ***} Response status (logged in, wrong or error)\n",
    "/predict_no_login: POST {'feature1': 'value1', 'feature2': 'value2', ...} (debug, no login needed) Response prediction\n",
    "```\n",
    "\n",
    "The `server` module contains the app, we can use a few rows of data to make a demo prediction, to be able to keep our user authentication, we must also create a session first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python server.py  # run separatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> 1.0\n",
      "\n",
      "<Response [200]> 0.0\n",
      "\n",
      "<Response [200]> 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from client import *\n",
    "session = requests.session()\n",
    "path = 'data/car_loan_trainset.csv'\n",
    "model_save_path = 'saved/model_demo'\n",
    "\n",
    "df = pd.read_csv(path).head(3)  # test with 3 rows\n",
    "df, x_columns = preprocess_pipeline(df, scale=True, scaler_path='saved/scaler.pkl')\n",
    "features = df[x_columns].to_numpy()\n",
    "predict(features, session, need_login=False)  # using debug mode without login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the 3 predictions from the server.\n",
    "\n",
    "_2. Set up basic-auth (username/password) on the endpoint_\n",
    "\n",
    "_3. Write tests_\n",
    "\n",
    "We first create a table on the DB to store user information (don't worry, password is encrypted):\n",
    "```CREATE TABLE users (\n",
    "username SERIAL PRIMARY KEY,\n",
    "password TEXT NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "We can use the function defined in the `client` module (which simply send `POST`). \n",
    "First, try register a new user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User registration successful!\n"
     ]
    }
   ],
   "source": [
    "register('demo_user', 'demo_pwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, try to submit a prediction without login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [401]> Please login first!\n",
      "<Response [401]> Please login first!\n",
      "<Response [401]> Please login first!\n"
     ]
    }
   ],
   "source": [
    "predict(features, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the prediction, so now try logging in with the wrong password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong user credentials\n"
     ]
    }
   ],
   "source": [
    "login('demo_user', '123', session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the right password that we just registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in\n"
     ]
    }
   ],
   "source": [
    "login('demo_user', 'demo_pwd', session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> 0.0\n",
      "\n",
      "<Response [200]> 1.0\n",
      "\n",
      "<Response [200]> 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(features, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try logging in again with the same user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already logged in.\n"
     ]
    }
   ],
   "source": [
    "login('demo_user', 'demo_pwd', session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try registering with the same username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username already exists, choose a different one!\n"
     ]
    }
   ],
   "source": [
    "register('demo_user', 'demo_pwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All registered data are stored on the Postgres DB:\n",
    "\n",
    "```SELECT * FROM users;```\n",
    "\n",
    "Result:\n",
    "\n",
    "```\n",
    "| \"username\"  | \"password\"                                                     |\n",
    "|-------------|----------------------------------------------------------------|\n",
    "| \"abc\"       | \"$2a$06$yy6rkHcjSineQqgERKOIleH.e31/aaux4BSQn2nb3MDkzzTYx1FaC\" |\n",
    "| \"newuser\"   | \"$2a$06$xCP.xE2r61ZMumoJPD9tTeElZ0aESkk.EI3qYVbxI0LqpI9zu95zG\" |\n",
    "| \"demo_user\" | \"$2a$06$nqdbEM.2Df5Qt3V9lRwD7exgnRGQt2GGJzbT6K1nmOnjwKnL5Zaze\" |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
